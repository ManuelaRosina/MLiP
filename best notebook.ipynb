{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#import os\n#os.sys.path.insert(0,'/kaggle/input/imagedataaugmentor2/image_data_augmentor.py')\n\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\nfrom tqdm.auto import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time, gc\nimport albumentations\nfrom PIL import Image, ImageOps, ImageEnhance\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.augmentations import functional as F\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.keras.utils.data_utils import Sequence\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfiles_train = [f'train_image_data_{fid}.parquet' for fid in range(1)]\ndata_dir = '../input/bengaliai-cv19'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 137\nWIDTH = 236","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE=64\nN_CHANNELS=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu')(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=128, kernel_size=(5, 5), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Flatten()(model)\nmodel = Dense(1024, activation = \"relu\")(model)\nmodel = Dropout(rate=0.3)(model)\ndense = Dense(512, activation = \"relu\")(model)\n\nhead_root = Dense(168, activation = 'softmax')(dense)\nhead_vowel = Dense(11, activation = 'softmax')(dense)\nhead_consonant = Dense(7, activation = 'softmax')(dense)\n\nmodel = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)\ntrain_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_3_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_4_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_5_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiOutputDataGenerator(Sequence):\n    def __init__(self, x_set, y_root, y_vowel, y_consonant, batch_size, augmentations, image_size):\n        self.x = x_set\n        self.y_root = y_root\n        self.y_vowel = y_vowel\n        self.y_consonant = y_consonant\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.augment = augmentations\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y_root = self.y_root[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y_vowel = self.y_vowel[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y_consonant = self.y_consonant[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        #batch_x = [x.reshape(HEIGHT, WIDTH) for x in batch_x]\n        #batch_x = [cv2.resize(image, (self.image_size, self.image_size)) for image in batch_x]\n        augmented = [self.augment(image=x)['image'].astype(np.float32) for x in batch_x]\n        labels = [batch_y_root, batch_y_vowel, batch_y_consonant]\n        return np.stack(augmented, axis=0), labels #[batch_y_root, batch_y_vowel, batch_y_consonant]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128 #256\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import (\n    Compose, HorizontalFlip, RandomBrightnessContrast, GridDistortion, ElasticTransform,\n    ToFloat, ShiftScaleRotate, CoarseDropout\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    CoarseDropout(max_holes=8, max_height=20, max_width=20, p=0.1),\n    ShiftScaleRotate(\n                shift_limit=0.0625,\n                scale_limit=0.1,\n                rotate_limit=30,\n                p=0.1),\n    GridDistortion(p=0.3)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\n\nfor i in range(4):\n    train_df = pd.merge(pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n    \n    # Visualize few samples of current training dataset\n    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n    count=0\n    for row in ax:\n        for col in row:\n            col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float32))\n            count += 1\n    plt.show()\n    \n    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    X_train = resize(X_train)/255\n    \n    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS).astype(np.float32)\n    \n    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n    \n    #Y_train_root = train_df['grapheme_root']\n    #Y_train_vowel = train_df['vowel_diacritic']\n    #Y_train_consonant = train_df['consonant_diacritic']\n\n    print(f'Training images: {X_train.shape}')\n    print(f'Training labels root: {Y_train_root.shape}')\n    print(f'Training labels vowel: {Y_train_vowel.shape}')\n    print(f'Training labels consonants: {Y_train_consonant.shape}')\n    #print(type(X_train))\n    # Divide the data into training and validation set\n    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n    del train_df\n    del X_train\n    del Y_train_root, Y_train_vowel, Y_train_consonant\n    \n    #print(x_train.dtype, y_train_root.shape, y_train_vowel.shape, y_train_consonant.shape)\n    #print(x_test[0].shape, y_test_root.shape, y_test_vowel.shape, y_test_consonant.shape)\n\n    # Data augmentation for creating more training data\n    datagen = MultiOutputDataGenerator(x_train, y_train_root, y_train_vowel, y_train_consonant, augmentations=AUGMENTATIONS_TRAIN, batch_size=batch_size, image_size=128)\n\n\n    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n    #datagen.fit(x_train, augment=True)\n    #label_csv = pd.DataFrame({'grapheme_root': y_train_root, 'vowel_diacritic': y_train_vowel, 'consonant_diacritic': y_train_consonant})\n    #dataset_show = BengaliDataset(label_csv, x_train, list(range(x_train.shape[0])), 'train', 'train', 128, transform=transforms_train)\n    #dataloader = DataLoader(dataset_show, batch_size=4, shuffle=False, num_workers=4)\n    \n    # Fit the model\n    history = model.fit_generator(datagen,\n                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                              steps_per_epoch=x_train.shape[0] // batch_size\n                            , callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant], workers=5)\n\n    histories.append(history)\n    \n    # Delete to reduce memory usage\n    del x_train\n    del x_test\n    del y_train_root\n    del y_test_root\n    del y_train_vowel\n    del y_test_vowel\n    del y_train_consonant\n    del y_test_consonant\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h5py\nfrom keras.models import load_model\nmodel.save('adapted_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor i in range(4):\n    df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS).astype(np.float32)\n    \n    preds = model.predict(X_test)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}